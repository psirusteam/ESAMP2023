---
Title: "Disaggregation of Estimates in Small Areas: A Bayesian Approach"
Subtitle: "ECLAC - Social Statistics Unit"
format:  
  beamer: default
incremental: false
aspectratio: 1610
    #theme: Berkeley
toc: true
Email: andres.gutierrez@cepal.org
editor_options:
  markdown:
    wrap: 90
---

```{r setup, include=FALSE}
library(printr)
library(ggplot2)
library(magrittr)
library(survey)
library(srvyr)
library(convey)
library(TeachingSampling)
library(printr)
library(furrr)
library(purrr)
library(tidyr)
library(knitr)
library(kableExtra)
library(bayesplot) 
library(posterior)
library(patchwork)
library(rstan)
library(bayesplot)
library(posterior)

select <- dplyr::select


knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
ggplot2::theme_set(theme_bw())
options(digits = 4)

tba <- function(dat, cap = NA) {
  kable(
    dat,
    format = "latex",
    digits =  4,
    booktabs = T,
    linesep = "",
    caption = cap
  ) %>%
    kable_styling(bootstrap_options = "striped", full_width = F) %>%
    kable_classic(full_width = F)
}


```

# Sustainable Development Goal

## 

```{r echo=FALSE, out.width = "500px", out.height="250px",fig.align='center'}
include_graphics("www/F_01_ODS.PNG")
```

## Some targets of SDG 2 (Zero Hunger)

By 2030, end hunger and ensure access to
all people, particularly the poor and those in
vulnerable situations, including infants under 1 year of age, to healthy, nutritious, and sufficient food all year round.

  - Prevalence of undernourishment.
  
  - Prevalence of moderate or severe food insecurity in the
   population, according to the Food Insecurity Experience Scale.

## Some targets of SDG 8 (Decent Work)

By 2030, achieve full and productive employment and decent work for all women and men, including
youth and persons with disabilities, and equal pay for equal work.

  - Unemployment rate, disaggregated by gender, age, and persons with
disabilities.


## Fundamental Principle of Data Disaggregation

  Indicators of the Sustainable Development Goals
  should be disaggregated, whenever relevant, by income,
  gender, age, race, ethnicity, migratory status, disability,
  and geographical location, or other characteristics, in accordance
  with the Fundamental Principles of Official Statistics.

**General Assembly Resolution - 68/261**

# Survey Limitations.

## What is the Coefficient of Variation?

The coefficient of variation is a measure of relative error for an
estimator and is defined as:

$$
cve\left(\hat{\theta}\right)=\frac{SE\left(\hat{\theta}\right)}{\hat{\theta}}
$$

It is often expressed as a percentage, even though it is not
bounded to the right, which makes it convenient when discussing
the precision of statistics derived from surveys.

## Alert Standards in Some Countries (Household Surveys)

```{r echo=FALSE, out.width = "400px", out.height="250px",fig.align='center', fig.cap= "Alerts on Coefficients of Variation"}
include_graphics("www/F_02_cve.PNG")
```

## Some Alerts Defined in the Publication

When the threshold of the coefficient of variation is exceeded, some of the following alerts may appear:

  -   Not published.
  
  -   Use with caution.
  
  -   Estimates require revisions, are not precise, and should be used with caution.
  
  -   Unreliable, less precise. 
  
  -   Does not meet publication standards. 
  
  -   With reservation, reference, questionable. 

  -   Very random values, poor estimation.
  
  
## Study Domains and Subpopulations of Interest

A survey is designed to generate accurate and reliable information within predefined study domains. However, there are subpopulations that the survey did not address in its design but for which greater precision is desired.

  -   Poverty incidence disaggregated by department or province (known and planned sample size).
  
  -   Unemployment rate disaggregated by gender (random but planned sample size).
  
  -   Net primary school attendance rate disaggregated by income quintiles (random sample size).

## Precision of Estimators

Because a survey is a partial investigation of a finite population, it's important to know that:

  -   Indicators are not calculated from a survey; they are estimated using survey data.
  
  -   It is necessary to calculate the degree of error resulting from the inability to conduct a comprehensive investigation. This error is known as sampling error.
  
  -   The precision of an estimator is dependent on the confidence interval.

A narrower interval results in greater precision and, therefore, lower sampling error.

## Effective Sample Size

  -   In household surveys with complex sampling designs, there is no sequence of variables that are independent and identically distributed.
  
  -   The sample $y_{1},\dots,y_{n}$ is not a vector in an n-dimensional space, where each component of the vector can vary independently.
  
  -   The final dimension of the vector ($y_{1},\dots,y_{n}$) is much smaller than n, due to hierarchical sampling and the relationship between the variable of interest and primary sampling units (PSUs).

## Effective Sample Size

The effective sample size is defined as follows:
$$
n_{effective}=\frac{n}{Deff}
$$

Where Deff is the design effect, which depends on: _1._ The average number of surveys conducted in each PSU. _2._ The correlation between the variable of interest and the same PSUs.

It can be considered that if the effective sample size is not greater than a threshold, then the figure should not be considered for publication.


## Degrees of Freedom

In subpopulations, degrees of freedom are not considered fixed but rather variable.

$$
df=\sum_{h=1}^{H}v_{h}\times\left(n_{Ih}-1\right)
$$

Note that $ν_h$ is an indicator variable that takes the value one if stratum $h$ contains one or more cases of the subpopulation of interest, and $n_{Ih}$ is the number of primary sampling units (PSUs) in the stratum. In the most general case, degrees of freedom are reduced to the following expression:

          df = #PSUs − #Stratum

# Introduction to Bayesian Thinking.


## Models of Areas with the **Tom Approach**

And you wake up one day...

-   You feel a little strange and weak. You go to the doctor, and they run some tests. One of them comes back positive for a very rare disease that only affects 0.1% of the population.

**Not good news.**

-   You go to the doctor's office and ask how specific the test is. They tell you it's very accurate; it correctly identifies 99% of the people who have the disease.

## And you meet Thomas...

```{r echo=FALSE, out.width = "500px", out.height="250px",fig.align='center'}
include_graphics("www/00_Intro_bayes/01_Fig_Ton.PNG")
```

## How does it work?

```{r echo=FALSE, out.width = "500px", out.height="250px",fig.align='center'}
include_graphics("www/00_Intro_bayes/02_Fig_bayes.png")
```

## How does it work?

```{r echo=FALSE, out.width = "500px", out.height="250px",fig.align='center'}
include_graphics("www/00_Intro_bayes/03_Fig_bayes2.PNG")
```

## And you seek a second opinion

-   This time the doctor orders you to retake the same test... and you test positive for that disease again.

- **And you wonder again:** _What is the probability that I have this disease?_

This time, you've updated your information about $Pr(E)$ because you've tested positive on a test

$$
Pr(E)= 0.09 \ And \ Pr(-E) = 0.91
$$

Therefore:

$$
Pr(E \mid ++) = 0.997 \approx 91\%
$$

## Elements of Bayes' Rule

In terms of inference for $\boldsymbol{\theta}$, it's necessary to find the distribution of the parameters conditioned on the observation of the data. To achieve this, you need to define the joint distribution of the variable of interest with the parameter vector.

$$
p(\boldsymbol{\theta},\boldsymbol{Y})=p(\boldsymbol{\theta})p(\boldsymbol{Y} \mid \boldsymbol{\theta})
$$

-   The distribution $p(\boldsymbol{\theta})$ is known as the prior distribution.

-   The term $p(\boldsymbol{Y} \mid \boldsymbol{\theta})$ is the sampling distribution, likelihood, or data distribution.

-   The distribution of the parameter vector conditioned on the observed data is given by

$$
p(\boldsymbol{\theta} \mid \boldsymbol{Y})=\frac{p(\boldsymbol{\theta},\boldsymbol{Y})}{p(\boldsymbol{Y})}=\frac{p(\boldsymbol{\theta})p(\boldsymbol{Y} \mid \boldsymbol{\theta})}{p(\boldsymbol{Y})}
$$


## Bayes' Rule

-   The term $p(\boldsymbol{\theta} \mid \boldsymbol{Y})$ is known as the ***posterior*** distribution.

-   The denominator does not depend on the parameter vector, and considering the observed data as fixed, it corresponds to a constant and can be omitted. Therefore,

$$
    p(\boldsymbol{\theta} \mid \boldsymbol{Y})\propto p(\boldsymbol{Y} \mid \boldsymbol{\theta})p(\boldsymbol{\theta})
$$

## Informative Prior Distribution for $\theta$

```{r echo=FALSE, out.width = "400px", out.height="250px",fig.align='center'}
include_graphics("www/00_Intro_bayes/04_Fig_bayes_previa.PNG")
```

## Non-Informative Prior Distribution for $\theta$

```{r echo=FALSE, out.width = "400px", out.height="250px",fig.align='center'}
include_graphics("www/00_Intro_bayes/05_Fig_bayes_previa2.PNG")
```

## Poisson Area Model 

Suppose $\boldsymbol{Y}=\{Y_1,\ldots,Y_n\}$ is a random sample of variables with a Poisson distribution with parameter $\theta$. The joint distribution function or likelihood function is given by
$$
p(\boldsymbol{Y} \mid \theta) = \prod_{i=1}^n\frac{e^{-\theta}\theta^{y_i}}{y_i!}I_{\{0,1,\ldots\}}(y_i)
$$

$$
= \frac{e^{-n\theta}\theta^{\sum_{i=1}^ny_i}}{\prod_{i=1}^ny_i!}I_{\{0,1,\ldots\}^n}(y_1,\ldots,y_n)
$$

where $\{0,1,\ldots\}^n$ denotes the Cartesian product $n$ times over the set $\{0,1,\ldots\}$. 

The parameter $\theta$ is restricted to the space $\Theta=(0,\infty)$.


## Distribución previa para $\theta$

-   La distribución previa del parámetro $\theta$ dada por

$$
p(\theta \mid \alpha,\beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1} e^{-\beta\theta}I_{(0,\infty)}(\theta).
$$

-   La distribución posterior del parámetro $\theta$ está dada por

$$
\theta \mid \boldsymbol{Y} \sim Gamma\left(\sum_{i=1}^ny_i+\alpha,n+\beta\right)
$$
## Prior Distribution for $\theta$

-   The prior distribution for the parameter $\theta$ is given by

$$
p(\theta \mid \alpha, \beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1} e^{-\beta\theta}I_{(0,\infty)}(\theta).
$$

-   The posterior distribution for the parameter $\theta$ is given by

$$
\theta \mid \boldsymbol{Y} \sim \text{Gamma}\left(\sum_{i=1}^ny_i+\alpha, n+\beta\right)
$$

## Estimation Process in **STAN**

Let $Y$ be the count of surveyed people living below the poverty line, expressed as a rate of (X) per 100 inhabitants, by administrative division of the country.

```{r,echo=TRUE}
dataPois <- readRDS("www/00_Intro_bayes/Poisson/dataPoisson.rds")
```

```{r,echo=FALSE}
tba(dataPois %>% head(10), cap = "People Count" )
```

## Histogram with People Count

```{r echo=FALSE, out.width = "400px", out.height="300px",fig.align='center',fig.cap="People count by administrative division"}
include_graphics("www/00_Intro_bayes/Poisson/02_Hist_pois.png")
```

## Model Written in `STAN` Code
\small
```
data {
  int<lower=0> n;      // Number of geographic areas 
  int<lower=0> y[n];   // Counts per area
  real<lower=0> alpha;
  real<lower=0> beta;
}
parameters {
  real<lower=0> theta;
}
model {
  y ~ poisson(theta);
  theta ~ gamma(alpha, beta);
}
generated quantities {
    real ypred[n];                    // Vector of length n
    for(ii in 1:n){
    ypred[ii] = poisson_rng(theta);
    }
}

```

## Preparing Data for `STAN` Code

-   Organizing data for STAN

```{r,eval=FALSE}
sample_data <- list(n = nrow(dataPois), y = dataPois$n, 
                    alpha = 0.001, beta = 0.001)
```

-   Running the `STAN` code

```{r, eval=FALSE}
stan_pois <- "www/00_Intro_bayes/Poisson/03_Poisson.stan"
model_poisson <-
  stan(
    file = stan_pois, data = sample_data,
    warmup = 500,
    iter = 1000,
    verbose = FALSE,   cores = 4
  )
saveRDS(model_poisson,
           "www/00_Intro_bayes/Poisson/model_poisson.rds")

```

## Results of the Estimation of Parameter $\theta$

```{r,eval=FALSE}
tabla_posi <- summary(model_poisson, 
                      pars =c("theta"))$summary
tabla_posi%>%tba()

```
\small
```{r, echo=FALSE}
tabla_posi <- readRDS("www/00_Intro_bayes/Poisson/04_tabla_theta.rds")
tabla_posi %>% tba() 
```

## Convergence of Chains for Parameter $\theta$

```{r,eval=FALSE}
posterior_theta <- as.array(model_poisson, pars = "theta")
p1 <- (mcmc_dens_chains(posterior_theta) +
    mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta)
```

```{r echo=FALSE, out.width = "650px", out.height="250px",fig.align='center', fig.cap="Chains for theta"}
knitr::include_graphics("www/00_Intro_bayes/Poisson/04_Pois_theta.png")
```

## Posterior Predictive Check

```{r, eval=FALSE}
y_pred_B<-as.array(model_poisson,pars ="ypred") %>% 
  as_draws_matrix()

rowsrandom<-sample(nrow(y_pred_B),100)

y_pred2<-y_pred_B[rowsrandom,]

p1<- ppc_dens_overlay(y =as.numeric(dataPois$n*100), y_pred2*100)


```

## Posterior Predictive Check

```{r echo=FALSE, out.width = "850px", out.height="250px",fig.align='center', fig.cap="Chains for theta"}
knitr::include_graphics("www/00_Intro_bayes/Poisson/05_ppc.png")
```

## Unit Model: Normal with Unknown Mean and Variance

- In the normal model, a set of independent and identically distributed variables $Y_1,\cdots,Y_n\sim N(\theta,\sigma^2)$ is considered.

- When both the mean and the variance of the distribution are unknown, different approaches are proposed to assign prior distributions to $\theta$ and $\sigma^2$ based on the problem context.

## Prior Distributions for $\theta$ and $\sigma^2$

Three possible assumptions about prior distributions for $\theta$ and $\sigma^2$ are described, considering independence and informativeness.

  - Assume that the prior distribution $p(\theta)$ is independent of the prior distribution $p(\sigma^2)$ and that both distributions are informative.

  - Assume that the prior distribution $p(\theta)$ is independent of the prior distribution $p(\sigma^2)$ and that both distributions are non-informative.

  - Assume that the prior distribution for $\theta$ depends on $\sigma^2$ and write it as $p(\theta \mid \sigma^2)$, while the prior distribution for $\sigma^2$ does not depend on $\theta$ and can be written as $p(\sigma^2)$.

The prior distribution is set for the parameter $\theta$ as $\theta \sim Normal(0,10000)$ and for the parameter $\sigma^2$ as $\sigma^2 \sim IG(0.0001,0.0001)$.

## Definition of the Normal Model

- The goal of the model is to estimate the average income of people, represented as $\bar{Y}_d = \frac{\sum_{U_d}y_{di}}{N_d}$.

- A way to estimate $\bar{Y}$ using $\hat{y}_{di}$, which is the expected value of $y_{di}$ under a probability measure induced by the model, is shown.

- Finally, the estimate of $\hat{\bar{Y}}_d = \frac{\sum_{U_{d}}\hat{y}_{di}}{N_d}$ is presented.

## Estimation Process

- To estimate the average income of people, i.e., 

$$\bar{Y}_d = \frac{\sum_{U_d}y_{di}}{N_d}$$

where $y_{di}$ is the income of each person. Note that

$$
\bar{Y}_d =  \frac{\sum_{s_d}y_{di} + \sum_{s^c_d}y_{di}}{N_d} 
$$

Now, the estimator of $\bar{Y}$ is given by: 

$$
\hat{\bar{Y}}_d = \frac{\sum_{s_d}y_{di} + \sum_{s^c_d}\hat{y}_{di}}{N_d}
$$


## Estimation Process 

Now, it is possible to assume that $\hat{y}_{di}$ is the conditional expectation given the modeling, that is

$$\hat{y}_{di}=E_{\mathscr{M}}\left(y_{di}\mid\boldsymbol{x}_{d},\boldsymbol{\beta}\right)$$,

where $\mathscr{M}$ refers to the probability measure induced by the modeling. Finally, it is found that

$$
\hat{\bar{Y}}_d = \frac{\sum_{U_{d}}\hat{y}_{di}}{N_d}
$$

## Estimation Process in **STAN**

Let $Y$ be the logarithm of income for an administrative division of the country. 

```{r}
dataNormal <- readRDS("www/00_Intro_bayes/Normal/01_dataNormal.rds")
tba(dataNormal %>% head(10), cap = "Logarithm of income" )
```

## Graphical Analysis of Logarithm of Income

```{r echo=FALSE, out.width = "500px", out.height="250px",fig.align='center'}
include_graphics("www/00_Intro_bayes/Normal/02_qqNormal.png")
```

## Model Written in `STAN` Code


::::  {.columns align=top .onlytextwidth}

::: {.column width="40%" align=center}
```
data {
  int<lower=0> n;
  real y[n];
}
parameters {
  real sigma;
  real theta;
}
transformed parameters {
  real sigma2;
  sigma2 = pow(sigma, 2);
}
```
:::

::: {.column width="60%" align=center}
```
model {
  y ~ normal(theta, sigma);
  theta ~ normal(0, 1000);
  sigma2 ~ inv_gamma(0.001, 0.001);
}
generated quantities {
    real ypred[n];                   
    for(kk in 1:n){
    ypred[kk] = normal_rng(theta,sigma);
}
}

```
:::

:::: 


## Preparing Data for the `STAN` Code

- Organizing data for `STAN`

```{r, eval=FALSE}
sample_data <- list(n = nrow(dataNormal),
                    y = dataNormal$logIngreso)
```

- Running `STAN` from `R` using the **rstan** library

```{r, eval = FALSE, message=FALSE}
NormalMeanVar  <- "www/00_Intro_bayes/Normal/03_NormalMeanVar.stan" 
model_NormalMedia <- stan(
  file = NormalMeanVar, 
  data = sample_data,   
  warmup = 500,         
  iter = 1000,
  verbose = FALSE, cores = 4              
)
saveRDS(model_NormalMedia,
        "www/00_Intro_bayes/Normal/model_NormalMedia2.rds")
```

## Results of the Estimation of the Parameters $\theta$ and $\sigma^2$ are:

```{r, eval=FALSE}
tabla_Nor2 <- summary(model_NormalMedia, 
        pars = c("theta", "sigma2", "sigma"))$summary
```

\small

```{r, echo=FALSE}
tabla_Nor2 <- readRDS("www/00_Intro_bayes/Normal/04_tabla_Normal.rds")
tabla_Nor2 %>% tba() 
```

## Convergence of Chains for Parameter $\theta$

```{r,eval=FALSE}
posterior_theta <- as.array(model_NormalMedia, pars = "theta")
(mcmc_dens_chains(posterior_theta) +
    mcmc_areas(posterior_theta) ) / 
  mcmc_trace(posterior_theta)

```

```{r echo=FALSE, out.width = "400px", out.height="250px",fig.align='center', fig.cap="Chains for theta"}
knitr::include_graphics("www/00_Intro_bayes/Normal/05_Normal_theta.png")
```

## Convergence of Chains for Parameter $\sigma^2$


```{r,eval=FALSE}
posterior_sigma2 <- as.array(model_NormalMedia, pars = "sigma2")
(mcmc_dens_chains(posterior_sigma2) +
    mcmc_areas(posterior_sigma2) ) / 
  mcmc_trace(posterior_sigma2)
```

```{r echo=FALSE, out.width = "400px", out.height="250px",fig.align='center', fig.cap="Chains for sigma2"}
knitr::include_graphics("www/00_Intro_bayes/Normal/06_Normal_sigma2.png")
```

## Posterior Predictive Check of Income

```{r,eval=FALSE}

y_pred_B <- as.array(model_NormalMedia, pars = "ypred") %>%
  as_draws_matrix()

rowsrandom <- sample(nrow(y_pred_B), 100)

y_pred2 <- y_pred_B[rowsrandom,]

ppc_dens_overlay(
  y = as.numeric(exp(dataNormal$logIngreso) - 1), y_pred2) + 
  xlim(0, 5000000)
```

## Posterior Predictive Check of Income

```{r echo=FALSE, out.width = "450px", out.height="250px",fig.align='center', fig.cap="PPC for income"}
knitr::include_graphics("www/00_Intro_bayes/Normal/06_PPC_ingreso.png")
```


## Linear Models.

Linear regression is the basic technique in econometric analysis. Through this technique, we aim to determine linear dependence relationships between a dependent variable, or endogenous variable, and one or more explanatory variables, or exogenous variables.

## Bayesian Linear Models

First, note that the particular interest lies in the distribution of the vector of n random variables $\boldsymbol{Y} = (Y_1, \ldots, Y_n)'$ conditioned on the matrix of auxiliary variables $\boldsymbol{X}$ and indexed by the vector of parameters of interest $\boldsymbol{\beta} = (\beta_1, \ldots, \beta_q)'$ given by $p(\boldsymbol{Y} \mid \boldsymbol{\beta}, \boldsymbol{X}$.

The basic and classical model assumes that the likelihood for the variables of interest is:

$$
\boldsymbol{Y} \mid \boldsymbol{\theta}, \sigma^2, \boldsymbol{X} \sim \text{Normal}_n(\boldsymbol{X}\boldsymbol{\beta}, \sigma^2\boldsymbol{I}_n)
$$

where $\boldsymbol{I}_n$ denotes the identity matrix of order $n \times n$. Of course, the normal model is not the only one that can be postulated as the likelihood for the data.

## Independent Parameters

Assuming that the parameters are independent a priori, meaning that the joint prior distribution is given by:

$$
p(\boldsymbol{\beta},\sigma^2) = p(\boldsymbol{\beta})p(\sigma^2)
$$

Naturally, the prior distribution for the parameter vector $\boldsymbol{\beta}$ is normal. However, this time, the variance-covariance matrix will not depend on the other parameter $\sigma^2$. So, you have:

$$
\boldsymbol{\beta} \sim \text{Normal}_q(\boldsymbol{b},\boldsymbol{B})
$$

Similarly, the parameter $\sigma^2$ does not depend on $\boldsymbol{\beta}$, and you can assign it the following prior distribution:

$$
\sigma^2 \sim Inverse-Gamma\left(\frac{n_0}{2},\frac{n_0\sigma^2_0}{2}\right)
$$

